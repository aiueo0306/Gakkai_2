
from feedgen.feed import FeedGenerator
from datetime import datetime, timezone
from urllib.parse import urljoin
import os
import re
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError

BASE_URL = "https://jasweb.or.jp/"
GAKKAI = "æ—¥æœ¬å–˜æ¯å­¦ä¼š"

SELECTOR_TITLE = "dl dd"
title_selector = "a"
title_index = 0
href_selector = "a"
href_index = 0
SELECTOR_DATE = "dl dt"
date_selector = ""
date_index = 0
year_unit = "."
month_unit = "."
day_unit = ""
date_format = f"%Y{year_unit}%m{month_unit}%d{day_unit}"
date_regex = rf"(\d{{2,4}}){year_unit}(\d{{1,2}}){month_unit}(\d{{1,2}}){day_unit}"


def generate_rss(items, output_path, BASE_URL, gakkai_name):
    fg = FeedGenerator()
    fg.title(f"{gakkai_name}ãƒˆãƒ”ãƒƒã‚¯ã‚¹")
    fg.link(href=BASE_URL)
    fg.description(f"{gakkai_name}ã®æœ€æ–°ãƒˆãƒ”ãƒƒã‚¯æƒ…å ±")
    fg.language("ja")
    fg.generator("python-feedgen")
    fg.docs("http://www.rssboard.org/rss-specification")
    fg.lastBuildDate(datetime.now(timezone.utc))

    for item in items:
        entry = fg.add_entry()
        entry.title(item['title'])
        entry.link(href=item['link'])
        entry.description(item['description'])

        if item['pub_date'] is not None:
            guid_value = f"{item['link']}#{item['pub_date'].strftime('%Y%m%d')}"
            entry.guid(guid_value, permalink=False)
            entry.pubDate(item['pub_date'])
        else:
            # æ—¥ä»˜ãŒãªã„å ´åˆã¯ãƒªãƒ³ã‚¯ãã®ã‚‚ã®ã‚’GUIDã«ã—ã¦permalink=True
            entry.guid(item['link'], permalink=True)
            # pubDateã¯è¨­å®šã—ãªã„

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    fg.rss_file(output_path)
    print(f"\nâœ… RSSãƒ•ã‚£ãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†ï¼ğŸ“„ ä¿å­˜å…ˆ: {output_path}")

def extract_items(page):
    # iframeã‚’å¾…æ©Ÿã—ã¦å–å¾—
    page.wait_for_selector("iframe", timeout=10000)
    iframe_element = page.locator("iframe").first.element_handle()

    if iframe_element is None:
        print("âš  iframeãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return []

    frame = iframe_element.content_frame()
    if frame is None:
        print("âš  iframeã®ä¸­èº«ï¼ˆframeï¼‰ãŒã¾ã èª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        return []

    frame.wait_for_selector(SELECTOR_TITLE , timeout=10000)

    blocks1 = frame.locator(SELECTOR_TITLE )
    count = blocks1.count()
    print(f"ğŸ“¦ ç™ºè¦‹ã—ãŸè¨˜äº‹æ•°: {count}")
    items = []

    blocks2 = frame.locator(SELECTOR_DATE)

    max_items = 10
    for i in range(min(count, max_items)):
        try:
            block1 = blocks1.nth(i)
            block2 = blocks2.nth(i)

            if title_selector:
                title_elem = block1.locator(title_selector).nth(title_index)
                title = title_elem.inner_text().strip()
            else:
                title = block1.inner_text().strip()
            print(title)

            # URL
            if title_selector:
                try:
                    href = block1.locator(href_selector).nth(href_index).get_attribute("href")
                    full_link = urljoin(BASE_URL, href)
                except:
                    href = ""
                    full_link = BASE_URL
            else:
                try:
                    href = block1.get_attribute("href")
                    full_link = urljoin(BASE_URL, href)
                except:
                    href = ""
                    full_link = BASE_URL
            print(full_link)
            
            # æ—¥ä»˜
            # date_selector ãŒç©ºæ–‡å­—ã‚„ None ã§ãªã„å ´åˆ â†’ å­è¦ç´ æ¢ç´¢ã€ãã‚Œä»¥å¤–ã¯ãã®ã¾ã¾
            if date_selector:
                try:
                    date_text = block2.locator(date_selector).nth(date_index).inner_text().strip()
                except Exception as e:
                    print(f"âš  æ—¥ä»˜ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã«ã‚ˆã‚‹å–å¾—ã«å¤±æ•—: {e}")
                    date_text = ""
            else:
                try:
                    date_text = block2.inner_text().strip()
                except Exception as e:
                    print(f"âš  ç›´æ¥æ—¥ä»˜å–å¾—ã«å¤±æ•—: {e}")
                    date_text = ""
            print(date_text)
            match = re.search(date_regex, date_text)

            if match:
                year_str, month_str, day_str = match.groups()
                year = int(year_str)
                if year < 100:
                    year += 2000  # 2æ¡è¥¿æš¦ â†’ 2000å¹´ä»¥é™ã¨ä»®å®š
                pub_date = datetime(year, int(month_str), int(day_str), tzinfo=timezone.utc)
            else:
                print("âš  æ—¥ä»˜ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
                pub_date = None  # or continue

            items.append({
                "title": title,
                "link": full_link,
                "description": title,
                "pub_date": pub_date
            })

        except Exception as e:
            print(f"âš  è¡Œ{i+1}ã®è§£æã«å¤±æ•—: {e}")
            continue

    return items

with sync_playwright() as p:
    print("â–¶ ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•ä¸­...")
    browser = p.chromium.launch(headless=True)
    context = browser.new_context()
    page = context.new_page()

    try:
        print("â–¶ ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")
        page.goto(BASE_URL, timeout=30000)
        page.wait_for_load_state("load", timeout=30000)
    except PlaywrightTimeoutError:
        print("âš  ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        browser.close()
        exit()

    print("â–¶ è¨˜äº‹ã‚’æŠ½å‡ºã—ã¦ã„ã¾ã™...")
    items = extract_items(page)

    if not items:
        print("âš  æŠ½å‡ºã§ããŸè¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚HTMLæ§‹é€ ãŒå¤‰ã‚ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

    rss_path = "rss_output/Feed3.xml"
    generate_rss(items, rss_path,BASE_URL,GAKKAI)
    browser.close()
